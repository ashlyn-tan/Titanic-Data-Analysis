The dataset Titanic in Kaggle Competition is used for the analysis conducted below.
With a given test set named as 'test.raw' and a training set named as 'train.raw', 
both are first combined to form a full dataset 'train' for ease of computing new features and missing values.
After the necessary steps of feature engineering and computing for the missing values,
the first 891 rows originally from the training set is extracted out as 'traindata' with the unnecessary features dropped 
and the remaining is named as 'test'.

In this project, Logistic Regression is used in the prediction of the survival of the passengers in the dataset.
K-fold Cross Validation is first conducted with k=10, and an accuracy rate of 0.879120879120879 is predicted.
After predicting the surivival rate of the test dataset, an actual accuracy rate of 0.78469 is gotten instead.
This shows that the k fold CV conducted in this project over estimated the accuracy rate.

##CALLING LIBRARY
library(ggplot2)

##READING DATA
train.raw=read.csv('train.csv',header=T,na.strings=c(''))
test.raw=read.csv('test.csv',header=T,na.strings = c(''))
test.raw$Survived=0
train=rbind(train.raw,test.raw)

##DATA SUMMARY FOR TRAIN.RAW
str(train)
#To show the structure of the data
'data.frame':	1309 obs. of  12 variables:
 $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
 $ Survived   : num  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Name       : Factor w/ 1307 levels "Abbing, Mr. Anthony",..: 109 191 358 277 16 559 520 629 417 581 ...
 $ Sex        : Factor w/ 2 levels "female","male": 2 1 1 1 2 2 2 2 1 1 ...
 $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
 $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Ticket     : Factor w/ 929 levels "110152","110413",..: 524 597 670 50 473 276 86 396 345 133 ...
 $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Cabin      : Factor w/ 186 levels "A10","A14","A16",..: NA 82 NA 56 NA NA 130 NA NA NA ...
 $ Embarked   : Factor w/ 3 levels "C","Q","S": 3 1 3 3 3 2 3 3 3 1 ...

sapply(train,function(x) sum(is.na(x)))
PassengerId    Survived      Pclass        Name         Sex         Age       SibSp       Parch      Ticket        Fare 
          0           0           0           0           0         263           0           0           0           1 
      Cabin    Embarked 
       1014           2 
sapply(train,function(x) length(unique(x)))
PassengerId    Survived      Pclass        Name         Sex         Age       SibSp       Parch      Ticket        Fare 
       1309           2           3        1307           2          99           7           8         929         282 
      Cabin    Embarked 
        187           4 
#A summary of the data to show the number of missing or unique values

##FEATURE ENGINEERING
##Part 1: Name of Passengers
train$Name=as.character(train$Name)
for (i in 1:length(train$Name)){
  train$Salutation[i]=strsplit(train$Name, split='[,.]')[[i]][2]}
train$Salutation=sub(' ','',train$Salutation)
#To extract the salutation of each passenger
table(train$Salutation)
#To show the salutation in a table form with counts
rare=c('Capt','Col','Don','Jonkheer','Lady','Major','Mlle','Mme','Ms','Sir','the Countess','Dona')
train$Salutation[train$Salutation %in% rare ]='Rare'
train$Salutation=factor(train$Salutation)
#group the rare salutation into one category and change the salutation into factors

##Part 2: Family size
train$familysize=train$SibSp+train$Parch+1
#form the family size using number of siblings, parents and children
table(train$Survived,train$familysize)
#showing the number of passengers survived or not with relation to the family size
ggplot(train[1:891,],aes(x=familysize))+geom_bar(aes(fill=factor(Survived)),position='dodge')+scale_fill_manual(breaks=levels(train$Survived),values=c('black','red'))
![image](Titanic-Data-Analysis/Family Size with Survival rate.png)

#Can be seen that the proportion of survival is higher for family size of 2,3,4, while family size of 1,5,6,7,8 and 11 has higher mortality rate
train$famfac[train$familysize==1]=0
train$famfac[train$familysize>1 & train$familysize<5]=1
train$famfac[train$familysize>4]=2
train$famfac=factor(train$famfac)
#group the passengers with the respective family size and survival rate into famfac and change to factors

##Forecast missing values
#Part 1: Embarked
ggplot(train,aes(x=Embarked,y=Fare,fill=factor(Pclass)))+geom_boxplot()+geom_hline(aes(yintercept=80),colour='red', linetype='dashed') 
train$Embarked[is.na(train$Embarked)]='C'
#from the plot, the passengers with missing values of Embarked have the same fare as the median fare of passengers with Embarked value of C
#Part 2: Age
ggplot(train,aes(x=Age))+geom_bar(aes(fill=factor(famfac)))+scale_fill_manual(breaks=levels(train$famfac),values=c('black','red','blue','green'))
train$Age[is.na(train$Age)]=mean(train$Age[!is.na(train$Age)])

#Part 3: Fare
ggplot(train,aes(x=Embarked,y=Fare,fill=factor(Pclass)))+geom_boxplot()+geom_hline(aes(yintercept=80),colour='red', linetype='dashed') 
train$Fare[is.na(train$Fare)]=mean(train$Fare[train$Embarked=='S' & train$Pclass==3 & !is.na(train$Fare)])

#Dropping unneccessary variable
drop=c('Cabin','PassengerId','Name','Ticket')
traindata=train[1:891,!(names(train) %in% drop)]

#k-fold Cross validation, k=10
#seed is set to ensure result is replicable
# define training control
control= trainControl(method="repeatedcv", number=10, repeats=10)
# train the model
set.seed(1234)
model= train(Survived~., data=traindata, trControl=control, method="glm", family=binomial)
print(model)
Generalized Linear Model 

891 samples
 10 predictor
  2 classes: '0', '1' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 802, 802, 803, 802, 802, 802, ... 
Resampling results:

  Accuracy   Kappa    
  0.8259545  0.6273421

print(paste('Estimated Testing Accuracy Rate',meanerror))
"Estimated Testing Accuracy Rate 0.825954545454545"

#Fitting Logistic Regression model
modelglm=glm(Survived~.,family=binomial(link='logit'),data=traindata)
summary(modelglm)

Call:
glm(formula = Survived ~ ., family = binomial(link = "logit"), 
    data = traindata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6617  -0.5259  -0.3922   0.5444   2.4105  

Coefficients: (1 not defined because of singularities)
                   Estimate Std. Error z value Pr(>|z|)    
(Intercept)       18.314774 874.039640   0.021 0.983282    
Pclass            -0.995188   0.156936  -6.341 2.28e-10 ***
Sexmale          -16.828406 874.039255  -0.019 0.984639    
Age               -0.022245   0.009279  -2.397 0.016518 *  
SibSp             -0.068144   0.220477  -0.309 0.757266    
Parch              0.090188   0.218227   0.413 0.679404    
Fare               0.004154   0.002655   1.565 0.117657    
EmbarkedQ          0.086557   0.401887   0.215 0.829473    
EmbarkedS         -0.312782   0.252377  -1.239 0.215217    
SalutationMaster   3.561651   1.081521   3.293 0.000991 ***
SalutationMiss   -13.980604 874.039539  -0.016 0.987238    
SalutationMr      -0.069331   0.925007  -0.075 0.940253    
SalutationMrs    -13.362883 874.039584  -0.015 0.987802    
SalutationRare     0.263904   1.178270   0.224 0.822776    
SalutationRev    -14.855413 972.093676  -0.015 0.987807    
familysize               NA         NA      NA       NA    
famfac1           -0.316741   0.379977  -0.834 0.404518    
famfac2           -3.053323   1.118211  -2.731 0.006323 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1186.66  on 890  degrees of freedom
Residual deviance:  709.94  on 874  degrees of freedom
AIC: 743.94

Number of Fisher Scoring iterations: 15

#Prediction
test=train[892:1309,!(names(train) %in% drop)]
test=test[,!(names(test) %in% ex)]
resultglm=predict(modelglm,newdata=test,type='response')
resultglm=ifelse(resultglm>0.5,1,0)
write.csv(resultglm, "resultglm.csv")
print('Actual Testing Accuracy Rate 0.78469')

There is an improvement in the testing accuracy rate as compared to the previous glm with no feature engineering.
This shows that the additional features extracted are useful and will be kept as we try another model in the next section with random forest
